<!DOCTYPE html>
<html>

<head>
  <link rel="stylesheet" type="text/css" href="style.css">
</head>

<link href='https://fonts.googleapis.com/css?family=Roboto:300,400,500' rel='stylesheet' type='text/css'>

<div>
  <table width="100%" height="100%" background="http://www.image-sensors.com/ImageSensors/media/Image-Sensors-2015/News%20article%20image/Am_image.png" cellpadding="5" cellspacing="0">
    <tr>
      <td valign="bottom">
        <p style="color: white; font-family: 'Roboto', sans-serif; font-weight: 500; font-size: 50px">
        Introduction to Computer Vision
        </p>
      </td>
    </tr>
  </table>
</div>


			
<!-- Fixed navigation bar -->
<nav class="navbar navbar-inverse navbar-fixed-top">
  <div class="container-fluid">
    <div>
      <ul class="nav navbar-nav">
        <li><a href="index.html">Home</a></li>
        <li><a href="applications.html">Applications</a></li>
        <li class="dropdown">
          <a class="dropdown-toggle" data-toggle="dropdown" href="#">Learn More
            <span class="caret"></span></a>
            <ul class="dropdown-menu">
            <li><a href="tutorials.html">Tutorials</a></li>
            <li><a href="resources.html">Resources</a></li>
          </ul>
        </li>
        <li><a href="about.html">About</a></li>
        <br>
        <br>
      </ul>
    </div>
  </div>
</nav>



<h2 style="font-size: 28px; font-weight: 400">Tutorial 1: Pixels and Filters (Photo Processing)</h2>

<body style="font-family: 'Roboto', sans-serif; font-weight: 300; font-size: 16px">

  <p>Whether we're aware of it or not, computer vision is everywhere in our daily lives. For one, filtered photos are ubiquitous in our social media feeds, news articles, magazines, books&mdash;everywhere! Turns out, if you think of images as functions mapping locations in images to pixel values, then filters are just systems that form a new, and preferably enhanced, image from a combination of the original image's pixel values.</p>

  <p style="font-weight: 500">Images as Functions</p>
  <p>To better understand the inherent properties of images and the technical procedure used to manipulate and process them, we can think of an image, which is comprised of individual pixels, as a function, <i>f</i>. Each pixel also has its own value. For a grayscale image, each pixel would have an intensity between 0 and 255, with 0 being black and 255 being white. <i>f(x,y)</i> would then give the intensity of the image at pixel position <i>(x,y)</i>, assuming it is defined over a rectangle, with a finite range: <i>f: [a,b]</i> x <i>[c,d]</i> &#8594; [0, 255].</p>

  <figure>
    <p><img src="http://hosting.soonet.ca/eliris/remotesensing/LectureImages/pixel.gif" alt="Grayscale Pixel Values" style="width:200px;height:200px;"/></p>
  </figure>

  <p>A color image is just a simple extension of this. <i>f(x,y)</i> is now a vector of three values instead of one. Using an RGB image as an example, the colors are constructed from a combination of Red, Green, and Blue (RGB). Therefore, each pixel of the image would have three channels and be represented as a 1x3 matrix. Since the three colors have integer values from 0 to 255, there would be a total of 256*256*256 = 16,777,216 combinations or color choices.</p>

  <figure>
    <img src="Pictures1/function.png" alt="Color Image as a Function" style="width:254px;height:156px;"/>
    <figcaption><font size="2"><i>f(x,y)</i> can be represented as three functions merged together.</font></figcaption>
  </figure>

  <figure>
    <img src="Pictures1/colorpixels.png" alt="Color Pixel Values" style="width:309.2px;height:176px;"/>
  </figure>

  <p>An image, then, can be represented as a matrix of pixel values.</p>

  <figure>
    <img src="Pictures1/imagematrix.png" alt="Matrix of Pixel Values" style="width:422.4px;height:174px;"/>
  </figure>

  <p style="font-weight: 500">Image Processing</p>
  <p>There are two main types of image processing: image filtering and image warping. Image filtering changes the range (i.e. the pixel values) of an image, so the colors of the image are altered without changing the pixel positions, while image warping changes the domain (i.e. the pixel positions) of an image, where points are mapped to points without changing the colors.</p>

  <figure>
    <img src="Pictures1/processing.png" alt="Image Processing" style="width:625px;height:243px"/>
  </figure>

  <p>We will examine more closely image filtering. The goal of using filters is to modify or enhance image properties and/or to extract valuable information from the pictures such as edges, corners, and blobs (See Tutorial 2 to learn more about finding features.). Here are some examples of what applying filters can do to make images more visually appealing.</p>

  <figure>
    <img src="Pictures1/filters.png" alt="Image Processing" style="width:557px;height:368px"/>
  </figure>

  <p>Two commonly implemented filters are the moving average filter and image segmentation.</p>

  <p>The moving average filter replaces each pixel with the average pixel value of it and a neighborhood window of adjacent pixels. The effect is a more smooth image with sharp features removed.</p>

  <p>If we used a 3x3 neighboring window:</p>
  <figure>
    <img src="gifs/moving average.gif" alt="Moving Average" style="width:480px;height:326px"/>
  </figure>

  <figure>
    <img src="Pictures1/lena_movingaverage.png" alt="Moving Average" style="width:397px;height:199px"/>
  </figure>

  <p>Note the edge artifact.*</p>

  <p>Image segmentation is the partitioning of an image into regions where the pixels have similar attributes, so the image is represented in a more simplified manner, and so we can then identify objects and boundaries more easily. There are multiple ways, which will be discussed in detail in Tutorial 3, to perform segmentation. Here, we will look at one simple way it can be implemented based on thresholding. In this example, all pixels with an intensity greater than 100 are replaced with a white pixel (intensity 255) and all others are replaced with a black pixel (intensity 0).</p>

  <figure>
    <img src="Pictures1/lena_segmentation.png" alt="Segmentation" style="width:323.6px;height:217.8px"/>
  </figure>

  <p style="font-weight: 500">Convolution in 2D</p>

  <p>Frequently used for image processing, like smoothing and sharpening images and detecting edges, convolution in 2D operates on two images, with one functioning as the input image and the other, called the kernel, serving as a filter. It expresses the amount overlap of one function as it is shifted over another function, as the output image is produced by sliding the kernel over the input image.</p>

  <p>No change:</p>
  <figure>
    <img src="http://i.picasion.com/pic81/aa04a5831027c71f58eea1c074cc143e.gif" width="450" height="136" alt="No Change"/>
  </figure>

  <p>Shifted right by one pixel:</p>
  <figure>
    <img src="http://i.picasion.com/pic81/1e9f438b81ea70c11088e49b0bfc8bb8.gif" width="450" height="136" alt="Shifted Right"/>
  </figure>

  <p>Blurred:</p>
  <figure>
    <img src="http://i.picasion.com/pic81/97a0ff4b81db67b3aa45d0e62975b9c9.gif" width="450" height="136" alt="Shifted Right"/>
  </figure>

  <p>Note the edge artifact.*</p>

  <p>Sharpening filter:</p>
  <p>A sharpening filter can be broken down into two steps: It takes a smoothed image, subtracts it from the original image to obtain the "details" of the image, and adds the "details" to the original image.</p>

  <p>Step 1: Original - Smoothed = "Details"</p>
  <figure>
    <img src="Pictures1/sharpening1.png" alt="Sharpening" style="width:641px;height:310px"/>
  </figure>

  <p>Step 2: Original + "Details" = Sharpened</p>
  <figure>
    <img src="Pictures1/sharpening2.png" alt="Sharpening" style="width:642px;height:255px"/>
  </figure>

  <p>Result:</p>
  <figure>
    <img src="http://i.picasion.com/pic81/c784276ddf3c58807eef8808ffb93177.gif" alt="Sharpening" style="width:450" height="136"/>
  </figure>

  <p>For a more formal definition of convolution, click <a href="http://www.cs.cornell.edu/courses/cs1114/2013sp/sections/s06_convolution.pdf">here</a></p>

  <p><font size="2">*Often times, applying these filters, as seen with the moving average, blurring, and sharpening filters, will produce unwanted artifacts along the edges of the images. To rid of these artifacts, zero padding, edge value replication, mirror extension, or other methods can be used.</font></p>

  <p style="font-weight: 500">Correlation</p>
  <p>While a convolution is a filtering operation, correlation measures the similarity of two data sets, comparing two input signals as they are shifted by one another. When two signals match, the correlation result is maximized.</p>

  <p>One application is is a vision system for a TV remote control with template matching. Based on the hand position of a user, his or her hand acts as a remote control to switch channels, increase or decrease volume, etc.</p>

  <p style="font-weight: 500">Edge Detection</p>
  <p>In computer vision, edges are sudden discontinuities in an image, which can arise from surface normal, surface color, depth, illumination, or other discontinuities. Edges are important for two main reasons. 1) Most semantic and shape information can be deduced from them, so we can perform object recognition and analyze perspectives and geometry of an image. 2) They are a more compact representation than pixels.</p>

  <p>We can pinpoint where edges occur from an image's intensity profile along a row or column of the image. Wherever there is a rapid change in the intensity function indicates an edge, as seen where the function's first derivative has a local extrema.</p>

  <figure>
    <img src="Pictures1/edgedetection.png" alt="Edge Detection" style="width:497px" height="218px"/>
  </figure>

  <p>An image gradient, which is a generalization of the concept of derivative to more than one dimension, points in the direction where intensity increases the most. If the gradient is &#8711;<i>f</i> = [<sup>&#948;<i>f</i></sup>&frasl;<sub>&#948;<i>x</i></sub>,<sup>&#948;<i>f</i></sup>&frasl;<sub>&#948;<i>y</i></sub>], then the gradient direction would be &#952; = tan<sup>-1</sup>(<sup>&#948;<i>f</i></sup>&frasl;<sub>&#948;<i>y</i></sub>/<sup>&#948;<i>f</i></sup>&frasl;<sub>&#948;<i>x</i></sub>), and the edge strength would be the gradient magnitude: &#124;&#124;&#8711;<i>f</i>&#124;&#124; = &radic;<span style="text-decoration: overline">(&#948;<i>f</i>/&#948;<i>x</i>)<span style="font-size: 12px;vertical-align:+30%;">2</span>+(&#948;<i>f</i>/&#948;<i>y</i>)<span style="font-size: 12px;vertical-align:+30%;">2</span></span>.</p>

  <p>However, plotting the pixel intensities often results in noise, making it impossible to identify where an edge is by only taking the first derivative of the function.</p>

  <figure>
    <img src="Pictures1/noise.png" alt="Noise" style="width:495px" height="305px"/>
  </figure>

  <p>If we apply a filter that is a derivative of a Gaussian function, we can eliminate the image noise and effectively locate edges.</p>
  <figure>
    <img src="Pictures1/kernel.png" alt="Noise" style="width:495px" height="292px"/>
  </figure>

  <p>Building off of this procedure, we can design an edge detector. The optimal edge detector must be accurate, minimizing the number of false positives and false negatives; have precise localization, pinpointing edges at the positions where they actually occur; and have single response, ensuring that only one edge is found where there only is one edge.</p>

  <figure>
    <img src="Pictures1/detector.png" alt="Detector" style="width:511px" height="213px"/>
  </figure>

  <p>The canny edge detector is arguably the most commonly used edge detector in the field. It detects edges by:</p>
  <ol>
    <li>Applying the x and y derivatives of Gaussian to the image to eliminate noise, improve localization, and have single response.</li>
    <figure>
      <img src="Pictures1/canny1.png" alt="Gaussian" style="width:236px" height="177px"/>
    </figure>
    <li>Finding the magnitude and orientation of the gradient at each pixel.</li>
    <li>Performing non-maximum suppression, which thins the edges down to a single pixel in width, since the extracted edge from the gradient after step 2 would be quite blurry and since there can only be one accurate response.</li>
    <figure>
      <img src="Pictures1/canny2.png" alt="Non-maximum Suppression" style="width:398px" height="177px"/>
    </figure>
    <li>Thresholding and linking, also known as hysteresis, to determine the weak and strong edge pixels by defining a low and a high threshold, respectively, and then to link the edge curves with the high threshold first to start with the strong edge pixels, continuing the curves with the low threshold.</li>
  </ol>

  <p>Final result:</p>
  <figure>
      <img src="Pictures1/canny3.png" alt="Final Result" style="width:398px" height="212px"/>
  </figure>

  <p>The Gaussian kernel size, &#963;, also affects the edges detected. If &#963; is large, the more obvious, defining edges of the picture are retrieved. Conversely, if &#963; is small, the finer edges are picked out as well.</p>
  <figure>
      <img src="Pictures1/canny4.png" alt="Final Result" style="width:398px" height="150px"/>
  </figure>

  <p>To read more about the canny edge detector, click <a href="http://cmp.felk.cvut.cz/~cernyad2/TextCaptchaPdf/A%20Computational%20Approach%20to%20Edge%20Detection.pdf">here</a></p>

  <p>RANSAC: RANdom SAmple Consensus</p>
  <p>Line fitting is important in edge detection since many objects are characterized by straight lines. However, edge detection does not always suffice since there can be extra edge points, muddling up which model would be the best, missing parts of lines, and noise. Thus, Fischler and Bolles developed the RANSAC algorithm, which determines a best fit line given a data set and avoids the effect of outliers by finding inliers. Given a scatterplot and a certain threshold, RANSAC randomly selects a sample of points, counts the number of inliers within the threshold, and repeats this process until the maximum number of inliers is hit.</p>

  <img src="http://i.freegifmaker.me/1/4/5/1/1/8/14511885661568723.gif?1451188567" alt="RANSAC"/>
  <p>You can learn more about RANSAC <a href="http://dl.acm.org/citation.cfm?id=358692">here</a></p>

<a href="tutorials.html">Back</a>

</body>

</html>